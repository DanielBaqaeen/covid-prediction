---
title: "Project1"
---

```{r setup}
# Imported the data set, classified the predictors into classes of 0 and 1. Also, omitted data points that are labeled as unknown for the predictors age group and race and ethnicity

data=read.csv('COVID-19_project1.csv',header=TRUE)
newdata = data[, -c(1:3)]
newdata$sex <- ifelse(newdata[,2]=="Female", 1, 0)
newdata$hosp_yn <- ifelse(newdata[,5]=="Yes", 1, 0)
newdata$icu_yn <- ifelse(newdata[,6]=="Yes", 1, 0)
newdata$death_yn <- ifelse(newdata[,7]=="Yes", 1, 0)
newdata$medcond_yn <- ifelse(newdata[,8]=="Yes", 1, 0)
newdata$current_status <- ifelse(newdata[,1]=="Laboratory-confirmed case", 1, 0)

newdata <- newdata[!newdata$age_group =="Unknown",]
newdata <- newdata[!newdata$Race.and.ethnicity..combined. =="Unknown",]

# Classified the age group predictors into 9 unique classes

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="0 - 9 Years"){
    newdata$age_group[i] = 1
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="10 - 19 Years"){
    newdata$age_group[i] = 2
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="20 - 29 Years"){
    newdata$age_group[i] = 3
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="30 - 39 Years"){
    newdata$age_group[i] = 4
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="40 - 49 Years"){
    newdata$age_group[i] = 5
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="50 - 59 Years"){
    newdata$age_group[i] = 6
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="60 - 69 Years"){
    newdata$age_group[i] = 7
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="70 - 79 Years"){
    newdata$age_group[i] = 8
  }
}

for(i in 1: 192365)
{
  if(newdata$age_group[i] =="80+ Years"){
    newdata$age_group[i] = 9
  }
}

# Classified the race and ethnicity predictors into 6 unique classes

for(i in 1: 192365)
{
  if(newdata$Race.and.ethnicity..combined.[i] =="American Indian/Alaska Native, Non-Hispanic"){
    newdata$Race.and.ethnicity..combined.[i] = 1
  }
}

for(i in 1: 192365)
{
  if(newdata$Race.and.ethnicity..combined.[i] =="Asian, Non-Hispanic"){
    newdata$Race.and.ethnicity..combined.[i] = 2
  }
}

for(i in 1: 192365)
{
  if(newdata$Race.and.ethnicity..combined.[i] =="Black, Non-Hispanic"){
    newdata$Race.and.ethnicity..combined.[i] = 3
  }
}

for(i in 1: 192365)
{
  if(newdata$Race.and.ethnicity..combined.[i] =="Hispanic/Latino"){
    newdata$Race.and.ethnicity..combined.[i] = 4
  }
}

for(i in 1: 192365)
{
  if(newdata$Race.and.ethnicity..combined.[i] =="Multiple/Other, Non-Hispanic"){
    newdata$Race.and.ethnicity..combined.[i] = 5
  }
}

for(i in 1: 192365)
{
  if(newdata$Race.and.ethnicity..combined.[i] =="Native Hawaiian/Other Pacific Islander, Non-Hispanic"){
    newdata$Race.and.ethnicity..combined.[i] = 6
  }
}

for(i in 1: 192365)
{
  if(newdata$Race.and.ethnicity..combined.[i] =="White, Non-Hispanic"){
    newdata$Race.and.ethnicity..combined.[i] = 7
  }
}

```

```{r}
library(glmnet)
# Linear regression model predicting death_yn (dead or not) using all other variables. This model used the whole data set 
lr = glm(death_yn ~ ., family = "binomial", data=newdata)
summary(lr)

# Split the data into training and test data (70% was training and 30% was test)
index=sample(1:nrow(newdata), round(nrow(newdata)*0.7))
train=newdata[index, ]
test=newdata[-index, ]

# Linear regression model predicting death_yn (dead or not) using all other variables. This model used just the training set
lr.all = glm(death_yn ~ ., family = "binomial", data=train)
summary(lr.all)

# Correlation plot for all the data except age group and race and ethnicity predictors because they are not numeric
cor(train[, -c(3,4)])

# Linear regression model predicting death_yn (dead or not) using icu_yn (was in the icu or not) as an predictor. This model used just the training set

lr2 = glm(death_yn ~ icu_yn , family = "binomial", data=train)
summary(lr2)

# Linear regression model predicting death_yn (dead or not) using hosp_yn (was in the hospital or not) as an predictor. This model used just the training set

lr3 = glm(death_yn ~ hosp_yn , family = "binomial", data=train)
summary(lr3)


# Linear regression model predicting death_yn (dead or not) using age_group as an predictor. This model used just the training set

lr4 = glm(death_yn ~ age_group , family = "binomial", data=train)
summary(lr4)


# Linear regression model predicting death_yn (dead or not) using  age_group, sex, icu_yn,  hosp_yn, and medcond_yn (if they had a medical condition before or not) as predictors. This model used just the training set

lr5 = glm(death_yn ~ age_group + sex + icu_yn + hosp_yn + medcond_yn  , family = "binomial", data=train)
summary(lr5)

# Linear regression model predicting death_yn (dead or not) using  medcond_yn as the predictor. This model used just the training set

lr6 = glm(death_yn ~  medcond_yn  , family = "binomial", data=train)
summary(lr6)

# Linear regression model predicting death_yn (dead or not) using  Race.and.ethnicity..combined as the predictor. This model used just the training set

lr7 = glm(death_yn ~ Race.and.ethnicity..combined.  , family = "binomial", data=train)
summary(lr7)

#Confusion matrix for each of the linear regression models

glm.probs=predict(lr, test,type="response")
glm.pred=rep(0, 192365)
glm.pred[glm.probs>.5]= 1
table(glm.pred,newdata[,7])

glm.probs2=predict(lr.all, test,type="response")
glm.pred2=rep(0, 57709)
glm.pred2[glm.probs>.5]= 1
table(glm.pred2,test[,7])

glm.probs3=predict(lr2, test,type="response")
glm.pred3=rep(0, 57709)
glm.pred3[glm.probs3>.5]= 1
table(glm.pred3,test[,7])

glm.probs4=predict(lr3, test,type="response")
glm.pred4=rep(0, 57709)
glm.pred4[glm.probs4>.5]= 1
table(glm.pred4,test[,7])

glm.probs5=predict(lr4, test,type="response")
glm.pred5=rep(0, 57709)
glm.pred5[glm.probs5>.5]= 1
table(glm.pred5,test[,7])

glm.probs6=predict(lr5, test,type="response")
glm.pred6=rep(0, 57709)
glm.pred6[glm.probs5>.5]= 1
table(glm.pred6,test[,7])

glm.probs7=predict(lr6, test,type="response")
glm.pred7=rep(0, 57709)
glm.pred7[glm.probs5>.5]= 1
table(glm.pred7,test[,7])

glm.probs8=predict(lr7, test,type="response")
glm.pred8=rep(0, 57709)
glm.pred8[glm.probs5>.5]= 1
table(glm.pred8,test[,7])


```


In general, predicted not-dead (death_yn = 0) correctly significantly better than if dead (death_yn = 1)


```{r}
set.seed(1)

# Splitting the original test data into a smaller subset (91% of the test data)

index2=sample(1:nrow(test), round(nrow(test)*0.91))
train2=test[index2, ]
test2=test[-index2, ]

# Splitting the smaller subset into training and subsets (70% training and 30% test) 

index3=sample(1:nrow(train2), round(nrow(train2)*0.7))
train3=train2[index3, ]
test3=train2[-index3, ]

library(class)

# Finding the optimal K-value, 10 was the highest K-value that didn't give an error (too many tie in KNN, the value was found by trail and error) 

knn.data = rep(0,10)
for(i in 1:10){
knn.pred=knn(train3[,-7], test3[,-7] ,train3[,7],k=i)
knn.data[i] = mean(knn.pred==test3[,7])
}

optimal.k = which.max(knn.data)
optimal.k

#using the optimal K-value to predict death_yn (dead or not) using all other variables. Using the smaller subset of the test data
knn.pred = knn(train3[,-7], test3[,-7] ,train3[,7],k=optimal.k, use.all = FALSE)
table(knn.pred, test3[,7])

```


```{r}
# Splitting the data into increasingly smaller subsets and testing which K-value is the most optimal, This didn't affect our final KNN we decided to choose.This was an experiment to see the pattern when a larger range of K-values is used.) 

index4=sample(1:nrow(train3), round(nrow(train3)*0.9))
train4=train3[index4, ]
test4=train3[-index4, ]


index5=sample(1:nrow(test4), round(nrow(test4)*0.7))
train5=test4[index5, ]
test5=test4[-index5, ]

library(class)
knn.data = rep(0,450)

for(i in 1:450){
knn.pred=knn(train5[,-7], test5[,-7] ,train5[,7],k=i)
knn.data[i] = mean(knn.pred==test5[,7])
}

optimal.k.1 = which.max(knn.data)
optimal.k.1

knn.pred = knn(train5[,-7], test5[,-7] ,train5[,7],k=optimal.k.1, use.all = FALSE)
table(knn.pred, test5[,7])
```